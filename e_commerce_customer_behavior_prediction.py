# -*- coding: utf-8 -*-
"""E-commerce Customer Behavior Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QidhXrm_NQEyStvO-GUfKfyC74x0WIBM
"""

from google.colab import files

# Upload file
uploaded = files.upload()

import pandas as pd

df = pd.read_csv("E-commerce Customer Behavior - Sheet1.csv")
print(df.head())

print(df.info())  # Check column types and missing values
print(df.describe())  # Summary statistics
print(df.isnull().sum())  # Count missing values per column

# Drop rows with missing values in a specific column
df = df.dropna(subset=['Satisfaction Level'])

print(df.isnull().sum())  # Count missing values per column

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("E-commerce Customer Behavior - Sheet1.csv")

# Display dataset structure
print(df.head())

# Encode categorical variables using LabelEncoder
label_encoder = LabelEncoder()
df['Gender'] = label_encoder.fit_transform(df['Gender'])
df['City'] = label_encoder.fit_transform(df['City'])
df['Satisfaction Level'] = label_encoder.fit_transform(df['Satisfaction Level'])
# One-Hot Encoding for categorical variables (e.g., 'Membership Type')
df = pd.get_dummies(df, columns=['Membership Type', 'City', 'Gender'], drop_first=True)

# Now, the categorical variables are transformed into multiple binary columns

# Define features (X) and target (y)
X = df.drop('Satisfaction Level', axis=1)  # Features
y = df['Satisfaction Level']  # Target variable

# Split the dataset into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define features (X) and target (y)
X = df.drop('Satisfaction Level', axis=1)  # Features
y = df['Satisfaction Level']  # Target variable

# Split the dataset into training (80%) and testing (20%)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest model
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Classification report for detailed metrics (precision, recall, f1-score)
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()